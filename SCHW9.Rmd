---
title: "SCHW9"
author: "John Rothen"
date: "11/14/2020"
output: pdf_document
---


# Problem 1

## A

$f(x)$ can be split into two pieces; $x^2 /5$ and Normal(2,1) pdf. Thus we can let h(x) = $x^2 /5$. g(x) will be Normal(0,1). 

```{r}
temp <-function(x){
  f <- (1/(5*sqrt(2*pi)))*(x^2)*exp(-((x-2)^2)/2)
  return(f)
}
integrate(temp,-Inf, Inf)

set.seed(123)

n <- 1000
gsamp <- rnorm(n)
gx <- pnorm(gsamp)
hx <- (gsamp^2) /5
m<-mean((hx/gx)) #mean
m
v<-var((hx/gx)) #var
v
v+m^2
#over estimates the integral!

n <- 10000
gsamp <- rnorm(n)
gx <- pnorm(gsamp)
hx <- (gsamp^2) /5
m<-mean((hx/gx)) #mean
m
v<-var((hx/gx)) #var
v
v+m^2
#even further away, and variance is huge

n <- 50000
gsamp <- rnorm(n)
gx <- pnorm(gsamp)
hx <- (gsamp^2) /5
m<-mean((hx/gx)) #mean
m
v<-var((hx/gx)) #var
v
v+m^2
##even further away, and variance is huge
```

## B 

Let's try Normal(2,1), from the original pdf given. This will likely be a better option as it is closer to the original H(x) given, and this should reduce the variance via Jensen's inequality.

```{r}
n <- 1000
gsamp <- rnorm(n,2,1)
gx <- pnorm(gsamp)
hx <- (gsamp^2) /5
mean((hx/gx)) # very close to the real integral!
var((hx/gx)) # very small variance too!
```

## C

Note that $Var(X) = E(X^2) - E(X)^2$. Thus, the estimate of $E(X^2) = Var(X) + E(X)^2$.

```{r}
n <- 1000
gsamp <- rnorm(n,2,1)
gx <- pnorm(gsamp)
hx <- (gsamp^2) /5
var((hx/gx)) # variance
var((hx/gx)) + (mean((hx/gx)))^2 #E(x^2)

n <- 10000
gsamp <- rnorm(n,2,1)
gx <- pnorm(gsamp)
hx <- (gsamp^2) /5
var((hx/gx)) # variance
var((hx/gx)) + (mean((hx/gx)))^2#E(x^2)

n <- 50000
gsamp <- rnorm(n,2,1)
gx <- pnorm(gsamp)
hx <- (gsamp^2) /5
var((hx/gx)) # variance
var((hx/gx)) + (mean((hx/gx)))^2#E(x^2)
```

## D 

The estimates from part A are extremely large, mostly due to the extremely high variance. The estimates in part C provide much more realistic estimations, with very small variances. The adjusted method used in part C is considerably better due to the better choice of g(x).

# Problem 2

## A

The formula for S(t) can be better represented using the popular analytic solution $S_t = S_0 exp((\mu- \sigma^2/2)t + \sigma W_t)$. This is derived using Ito-Calculus.

```{r}
library(e1071)
st <- function(t,s0, r, sigma, n){
  w <- rwiener(1,n)
  s.t <- s0*exp((r-((sigma^2)/2))*t + (sigma*w))
  return(s.t)
}
```

## B

```{r}
set.seed(45)
s0 <- 1
r <- .05
T. <- 1
sigma <-.5
n=12
t=1
itn <- (t*T.)/n
#
ST<-0
SA <-0
SG <-0
PA<-0
PG<-0
PE<-0
#
k <- c(1.1,1.2,1.3,1.4,1.5)
for(i in 1:5000){
  samp<-st(t,s0,r,sigma,n)
  ST[i] <- samp[1]
  SA[i] <- mean(st(itn, s0,r,sigma,n))
  SG[i] <- (prod(st(itn, s0,r,sigma,n)))^(1/n)
}

for(i in 1:5){
  PA[i] <- exp(-r*T.)*max(SA - k[i])
  temp <-max(ST-k[i])
  PE[i] <- exp(-r*T.)*temp
  PG[i] <- exp(-r*T.)*max(SG - k[i])
}


#Cor of S(T) and PA
cor(ST[1:5],PA) # General trend is not obvious as K changes
cor(ST[1:3], PA[1:3]) #-.3
cor(ST[2:4], PA[2:4])# ~1
cor(ST[3:5], PA[3:5])#0


#Cor of PA and PE
cor(PA,PE)
#Cor of PA and PG
cor(PA,PG)
```

The correlation between PA/PE and PA/PG are consistent, and show perfect correlation, whereas for the correlation between S(T) and PA is negative for K= 1.1, 1.2, 1.3, but is positive for 1.3 to 1.5.


## C

```{r}
set.seed(412)
T. =1 
K =1.5
sigma = c(.2,.3,.4,.5)
ST<-0
SA <-0
SG <-0
PA<-0
PG<-0
PE<-0
for(i in 1:5000){
    samp<-st(t,s0,r,sigma[1],n)
    ST[i] <- samp[1]
    SA[i] <- mean(st(itn, s0,r,sigma[1],n))
    SG[i] <- (prod(st(itn, s0,r,sigma[1],n)))^(1/n)
}
PA1 <- exp(-r*T.)*max(SA-K)
PE1 <- exp(-r*T.)*max(ST-K)
PG1 <- exp(-r*T.)*max(SG-K)
ST1 <-ST[1]

for(i in 1:5000){
    samp<-st(t,s0,r,sigma[2],n)
    ST[i] <- samp[1]
    SA[i] <- mean(st(itn, s0,r,sigma[2],n))
    SG[i] <- (prod(st(itn, s0,r,sigma[2],n)))^(1/n)
}
PA2 <- exp(-r*T.)*max(SA-K)
PE2 <- exp(-r*T.)*max(ST-K)
PG2 <- exp(-r*T.)*max(SG-K)
ST2 <-ST[1]

for(i in 1:5000){
    samp<-st(t,s0,r,sigma[3],n)
    ST[i] <- samp[1]
    SA[i] <- mean(st(itn, s0,r,sigma[3],n))
    SG[i] <- (prod(st(itn, s0,r,sigma[3],n)))^(1/n)
}
PA3 <- exp(-r*T.)*max(SA-K)
PE3 <- exp(-r*T.)*max(ST-K)
PG3 <- exp(-r*T.)*max(SG-K)
ST3 <-ST[1]

for(i in 1:5000){
    samp<-st(t,s0,r,sigma[1],n)
    ST[i] <- samp[1]
    SA[i] <- mean(st(itn, s0,r,sigma[1],n))
    SG[i] <- (prod(st(itn, s0,r,sigma[1],n)))^(1/n)
}
PA4 <- exp(-r*T.)*max(SA-K)
PE4 <- exp(-r*T.)*max(ST-K)
PG4 <- exp(-r*T.)*max(SG-K)
ST4 <-ST[1]

PA <- c(PA1,PA2,PA3,PA4)
PE <- c(PE1,PE2,PE3,PE4)
PG <- c(PG1,PG2,PG3,PG4)
ST. <- c(ST1,ST2,ST3,ST4)
dat <- cbind(sigma,PA,PE,PG,ST.)
dat

#Cor change as ST and PA
cor(ST., PA)
cor(ST.[1:3], PA[1:3])
cor(ST.[2:4], PA[2:4])

#Cor for PA and PE
cor(PA,PE)
cor(PA[1:3],PE[1:3])
cor(PA[2:4],PE[2:4])

#Cor for PA PG
cor(PA,PG)
cor(PA[1:3],PG[1:3])
cor(PA[2:4],PG[2:4])
```

As sigma increases, the correlation between S(T) and PA decreases, and the same is true for PA and PG. The correlation between PA and PE increase as sigma increases.

## D 

```{r}
set.seed(412)
T. = c(.4,.7,1,1.3,1.6)
K =1.5
sigma = .5
ST<-0
SA <-0
SG <-0
PA<-0
PG<-0
PE<-0
for(j in 1:5){
  for(i in 1:5000){
      samp<-st(t,s0,r,sigma,n)
      ST[i] <- samp[1]
      SA[i] <- mean(st(itn, s0,r,sigma,n))
      SG[i] <- (prod(st(itn, s0,r,sigma,n)))^(1/n)
  }
  PA[j] <- exp(-r*T.[j])*max(SA-K)
  PE[j] <- exp(-r*T.[j])*max(ST-K)
  PG[j] <- exp(-r*T.[j])*max(SG-K)
  ST.[j] <- ST[1]
}

#PA ST
cor(PA,ST.)#overall -> increases until T=1.3, then drops significantly
cor(PA[1:3], ST.[1:3])#~zero
cor(PA[2:4], ST.[2:4])# .6
cor(PA[3:5], ST.[3:5])# -1
#PA PE
cor(PA,PE)#overall -> increases as T increases
cor(PA[1:3],PE[1:3])#-1
cor(PA[2:4],PE[2:4])# -.5
cor(PA[3:5],PE[3:5])# -.1
#PA PG
cor(PA,PG)#overall -> Decreases as T increases
cor(PA[1:3], PG[1:3])#.36
cor(PA[2:4], PG[2:4])# -.12
cor(PA[3:5], PG[3:5])# -.25
```

## E

```{r}
sigma=.4
T.=1
K=1.5
ST<-0
SA <-0
SG <-0
PA<-0
PG<-0
PE<-0
for(j in 1:12){
  for(i in 1:5000){
      samp<-st(t,s0,r,sigma,n)
      ST[i] <- samp[1]
      SA[i] <- mean(st(itn, s0,r,sigma,n))
      SG[i] <- (prod(st(itn, s0,r,sigma,n)))^(1/n)
  }
  PA[j] <- exp(-r*T.)*max(SA-K)
  PE[j] <- exp(-r*T.)*max(ST-K)
  PG[j] <- exp(-r*T.)*max(SG-K)
  ST.[j]<- ST[1]
}
#Variance of the Standard estimate for PA
sd(PA)
#Optimal minimized sd of control variate case
sqrt((1-cor(PA,PG)^2)*sd(PA)^2)
```

The variance for the control variate case is slightly smaller.